{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tests with K6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Lookup compatible configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get IE compute instance options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autobench.compute_manager import ComputeManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor</th>\n",
       "      <th>vendor_status</th>\n",
       "      <th>region</th>\n",
       "      <th>region_label</th>\n",
       "      <th>region_status</th>\n",
       "      <th>id</th>\n",
       "      <th>accelerator</th>\n",
       "      <th>num_gpus</th>\n",
       "      <th>memory_in_gb</th>\n",
       "      <th>gpu_memory_in_gb</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_size</th>\n",
       "      <th>architecture</th>\n",
       "      <th>status</th>\n",
       "      <th>price_per_hour</th>\n",
       "      <th>num_cpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-t4-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>nvidia-t4</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia T4</td>\n",
       "      <td>available</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-t4-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>192</td>\n",
       "      <td>64</td>\n",
       "      <td>nvidia-t4</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia T4</td>\n",
       "      <td>available</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a10g-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>nvidia-a10g</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia A10G</td>\n",
       "      <td>available</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a10g-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>186</td>\n",
       "      <td>96</td>\n",
       "      <td>nvidia-a10g</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia A10G</td>\n",
       "      <td>available</td>\n",
       "      <td>5.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a100-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>80</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vendor vendor_status     region region_label region_status  \\\n",
       "0    aws     available  us-east-1  N. Virginia     available   \n",
       "1    aws     available  us-east-1  N. Virginia     available   \n",
       "2    aws     available  us-east-1  N. Virginia     available   \n",
       "3    aws     available  us-east-1  N. Virginia     available   \n",
       "4    aws     available  us-east-1  N. Virginia     available   \n",
       "\n",
       "                             id accelerator  num_gpus  memory_in_gb  \\\n",
       "0    aws-us-east-1-nvidia-t4-x1         gpu         1            15   \n",
       "1    aws-us-east-1-nvidia-t4-x4         gpu         4           192   \n",
       "2  aws-us-east-1-nvidia-a10g-x1         gpu         1            30   \n",
       "3  aws-us-east-1-nvidia-a10g-x4         gpu         4           186   \n",
       "4  aws-us-east-1-nvidia-a100-x1         gpu         1           145   \n",
       "\n",
       "   gpu_memory_in_gb instance_type instance_size architecture     status  \\\n",
       "0                16     nvidia-t4            x1    Nvidia T4  available   \n",
       "1                64     nvidia-t4            x4    Nvidia T4  available   \n",
       "2                24   nvidia-a10g            x1  Nvidia A10G  available   \n",
       "3                96   nvidia-a10g            x4  Nvidia A10G  available   \n",
       "4                80   nvidia-a100            x1  Nvidia A100  available   \n",
       "\n",
       "   price_per_hour  num_cpus  \n",
       "0             0.5         3  \n",
       "1             3.0        46  \n",
       "2             1.0         6  \n",
       "3             5.0        46  \n",
       "4             4.0        11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_manager = ComputeManager()\n",
    "compute_manager.options.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   vendor            21 non-null     object \n",
      " 1   vendor_status     21 non-null     object \n",
      " 2   region            21 non-null     object \n",
      " 3   region_label      21 non-null     object \n",
      " 4   region_status     21 non-null     object \n",
      " 5   id                21 non-null     object \n",
      " 6   accelerator       21 non-null     object \n",
      " 7   num_gpus          21 non-null     int64  \n",
      " 8   memory_in_gb      21 non-null     int64  \n",
      " 9   gpu_memory_in_gb  21 non-null     int64  \n",
      " 10  instance_type     21 non-null     object \n",
      " 11  instance_size     21 non-null     object \n",
      " 12  architecture      21 non-null     object \n",
      " 13  status            21 non-null     object \n",
      " 14  price_per_hour    21 non-null     float64\n",
      " 15  num_cpus          21 non-null     int64  \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "compute_manager.options.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User specifies their desired inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR = \"aws\"\n",
    "REGION = \"us-east-1\"\n",
    "GPU_TYPES = [\"nvidia-a10g\", \"nvidia-l4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_instances = compute_manager.get_instance_details(\n",
    "    vendor=VENDOR, region=REGION, gpu_types=GPU_TYPES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possible_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, check if model will work on any of the desired instances, and if so, get each TGI config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "viable_instances = compute_manager.get_viable_instance_configs(\n",
    "    model_id=model_id, instances=possible_instances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tgi_config': TGIConfig(model_id='meta-llama/Meta-Llama-3-8B-Instruct', max_batch_prefill_tokens=8192, max_input_length=6000, max_total_tokens=6144, num_shard=1, quantize=None, estimated_memory_in_gigabytes=23.01),\n",
       "  'instance_config': ComputeInstanceConfig(id='aws-us-east-1-nvidia-a10g-x1', vendor='aws', vendor_status='available', region='us-east-1', region_label='N. Virginia', region_status='available', accelerator='gpu', num_gpus=1, memory_in_gb=30, gpu_memory_in_gb=24, instance_type='nvidia-a10g', instance_size='x1', architecture='Nvidia A10G', status='available', price_per_hour=1.0, num_cpus=6)},\n",
       " {'tgi_config': TGIConfig(model_id='meta-llama/Meta-Llama-3-8B-Instruct', max_batch_prefill_tokens=32768, max_input_length=6000, max_total_tokens=6144, num_shard=4, quantize=None, estimated_memory_in_gigabytes=98.36),\n",
       "  'instance_config': ComputeInstanceConfig(id='aws-us-east-1-nvidia-a10g-x4', vendor='aws', vendor_status='available', region='us-east-1', region_label='N. Virginia', region_status='available', accelerator='gpu', num_gpus=4, memory_in_gb=186, gpu_memory_in_gb=96, instance_type='nvidia-a10g', instance_size='x4', architecture='Nvidia A10G', status='available', price_per_hour=5.0, num_cpus=46)},\n",
       " {'tgi_config': TGIConfig(model_id='meta-llama/Meta-Llama-3-8B-Instruct', max_batch_prefill_tokens=32768, max_input_length=6000, max_total_tokens=6144, num_shard=4, quantize=None, estimated_memory_in_gigabytes=98.36),\n",
       "  'instance_config': ComputeInstanceConfig(id='aws-us-east-1-nvidia-l4-x4', vendor='aws', vendor_status='available', region='us-east-1', region_label='N. Virginia', region_status='available', accelerator='gpu', num_gpus=4, memory_in_gb=185, gpu_memory_in_gb=96, instance_type='nvidia-l4', instance_size='x4', architecture='Nvidia L4', status='available', price_per_hour=3.8, num_cpus=47)},\n",
       " {'tgi_config': TGIConfig(model_id='meta-llama/Meta-Llama-3-8B-Instruct', max_batch_prefill_tokens=8192, max_input_length=6000, max_total_tokens=6144, num_shard=1, quantize=None, estimated_memory_in_gigabytes=23.01),\n",
       "  'instance_config': ComputeInstanceConfig(id='aws-us-east-1-nvidia-l4-x1', vendor='aws', vendor_status='available', region='us-east-1', region_label='N. Virginia', region_status='available', accelerator='gpu', num_gpus=1, memory_in_gb=30, gpu_memory_in_gb=24, instance_type='nvidia-l4', instance_size='x1', architecture='Nvidia L4', status='available', price_per_hour=0.8, num_cpus=7)}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viable_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deploy LLM with TGI on Inference Endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewreed/Documents/success_projects/auto-bench/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autobench.deployment import Deployment\n",
    "from autobench.config import DeploymentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tgi_config': TGIConfig(model_id='meta-llama/Meta-Llama-3-8B-Instruct', max_batch_prefill_tokens=8192, max_input_length=6000, max_total_tokens=6144, num_shard=1, quantize=None, estimated_memory_in_gigabytes=23.01),\n",
       " 'instance_config': ComputeInstanceConfig(id='aws-us-east-1-nvidia-a10g-x1', vendor='aws', vendor_status='available', region='us-east-1', region_label='N. Virginia', region_status='available', accelerator='gpu', num_gpus=1, memory_in_gb=30, gpu_memory_in_gb=24, instance_type='nvidia-a10g', instance_size='x1', architecture='Nvidia A10G', status='available', price_per_hour=1.0, num_cpus=6)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viable_instance = viable_instances[0]\n",
    "viable_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAX_BATCH_PREFILL_TOKENS': '8192',\n",
       " 'MAX_INPUT_LENGTH': '6000',\n",
       " 'MAX_TOTAL_TOKENS': '6144',\n",
       " 'NUM_SHARD': '1',\n",
       " 'MODEL_ID': '/repository'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viable_instance[\"tgi_config\"].env_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfig(\n",
    "    tgi_config=viable_instance[\"tgi_config\"],\n",
    "    instance_config=viable_instance[\"instance_config\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting existing endpoint d30f3a82-95b9-44e8-b0ca-617d17cc\n",
      "Endpoint found.\n",
      "Endpoint status: running\n"
     ]
    }
   ],
   "source": [
    "# deployment = Deployment(deployment_config)\n",
    "\n",
    "deployment = Deployment(\n",
    "    deployment_config, existing_endpoint_name=\"d30f3a82-95b9-44e8-b0ca-617d17cc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d30f3a82-95b9-44e8-b0ca-617d17cc',\n",
       " 'running',\n",
       " 'https://ehdndzfif3v55m3w.us-east-1.aws.endpoints.huggingface.cloud')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment.deployment_id, deployment.endpoint.status, deployment.endpoint.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare ShareGPT data for realisitic inference workload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1999/2000 [00:00<00:00, 48205.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from autobench.config import DataConfig\n",
    "from autobench.data import BenchmarkDataset\n",
    "\n",
    "data_config = DataConfig()\n",
    "benchmark_dataset = BenchmarkDataset(data_config)\n",
    "benchmark_dataset.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benchmark_data/data.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config.file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run K6 Load Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, install the openai Python library by running\n",
    "# pip install openai\n",
    "\n",
    "import requests\n",
    "from huggingface_hub import get_token\n",
    "\n",
    "\n",
    "API_URL = f\"{deployment.endpoint.url}\" + \"/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {get_token()}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response\n",
    "\n",
    "\n",
    "output = query(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"How old is Statute of Liberty?\"}],\n",
    "        \"max_tokens\": 150,\n",
    "        \"model\": \"tgi\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1724960951,\"model\":\"/repository\",\"system_fingerprint\":\"2.2.1-dev0-sha-d9fbbaa\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The Statue of Liberty, officially named \\\\\"Liberty Enlightening the World,\\\\\" was dedicated on October 28, 1886. It was designed by French sculptor Fr\\xc3\\xa9d\\xc3\\xa9ric Auguste Bartholdi and was a gift from the people of France to the people of the United States.\\\\n\\\\nThe statue was disassembled and shipped from France to New York Harbor, where it was reassembled on Bedloe\\'s Island (now known as Liberty Island). The pedestal, designed by American architect Richard Morris Hunt, was completed in 1886, and the statue was officially dedicated on October 28, 1886.\\\\n\\\\nSo, as of 2023, the Statue of Liberty is 137 years old.\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":18,\"completion_tokens\":143,\"total_tokens\":161}}'\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autobench.runner import (\n",
    "    K6ConstantArrivalRateExecutor,\n",
    "    Scenario,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = K6ConstantArrivalRateExecutor(\n",
    "    pre_allocated_vus=10, rate_per_second=1, duration=\"10s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andrewreed/Documents/success_projects/auto-bench/benchmark_data/data.json'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_dataset.file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario(\n",
    "    host=deployment.endpoint.url,\n",
    "    executor=executor,\n",
    "    data_file=benchmark_dataset.file_path,\n",
    "    output_dir=os.path.abspath(\"../autobench/benchmark_results\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing scenario 79f58a9a-a2b8-47d7-a2ca-a4d60fc19eb4\n",
      "Running scenario 79f58a9a-a2b8-47d7-a2ca-a4d60fc19eb4\n",
      "\n",
      "          /\\      |‾‾| /‾‾/   /‾‾/   \n",
      "     /\\  /  \\     |  |/  /   /  /    \n",
      "    /  \\/    \\    |     (   /   ‾‾\\  \n",
      "   /          \\   |  |\\  \\ |  (‾)  | \n",
      "  / __________ \\  |__| \\__\\ \\_____/ .io\n",
      "\n",
      "     execution: local\n",
      "        script: /var/folders/w0/6t9rxkj97rv47l9sc0q22yth0000gn/T/autobench_emoz85zk_k6_script.js\n",
      "        output: json (/Users/andrewreed/Documents/success_projects/auto-bench/autobench/benchmark_results/scenario_79f58a9a-a2b8-47d7-a2ca-a4d60fc19eb4/results.json)\n",
      "\n",
      "     scenarios: (100.00%) 1 scenario, 10 max VUs, 40s max duration (incl. graceful stop):\n",
      "              * load_test: 1.00 iterations/s for 10s (maxVUs: 10, gracefulStop: 30s)\n",
      "\n",
      "\n",
      "running (01.0s), 01/10 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  10% ] 01/10 VUs  01.0s/10s  1.00 iters/s\n",
      "\n",
      "running (02.0s), 02/10 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  20% ] 02/10 VUs  02.0s/10s  1.00 iters/s\n",
      "\n",
      "running (03.0s), 03/10 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  30% ] 03/10 VUs  03.0s/10s  1.00 iters/s\n",
      "\n",
      "running (04.0s), 03/10 VUs, 1 complete and 0 interrupted iterations\n",
      "load_test   [  40% ] 03/10 VUs  04.0s/10s  1.00 iters/s\n",
      "\n",
      "running (05.0s), 04/10 VUs, 1 complete and 0 interrupted iterations\n",
      "load_test   [  50% ] 04/10 VUs  05.0s/10s  1.00 iters/s\n",
      "\n",
      "running (06.0s), 04/10 VUs, 2 complete and 0 interrupted iterations\n",
      "load_test   [  60% ] 04/10 VUs  06.0s/10s  1.00 iters/s\n",
      "\n",
      "running (07.0s), 03/10 VUs, 4 complete and 0 interrupted iterations\n",
      "load_test   [  70% ] 03/10 VUs  07.0s/10s  1.00 iters/s\n",
      "\n",
      "running (08.0s), 04/10 VUs, 4 complete and 0 interrupted iterations\n",
      "load_test   [  80% ] 04/10 VUs  08.0s/10s  1.00 iters/s\n",
      "\n",
      "running (09.0s), 02/10 VUs, 7 complete and 0 interrupted iterations\n",
      "load_test   [  90% ] 02/10 VUs  09.0s/10s  1.00 iters/s\n",
      "\n",
      "running (10.0s), 02/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test   [ 100% ] 02/10 VUs  10.0s/10s  1.00 iters/s\n",
      "\n",
      "running (11.0s), 02/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 02/10 VUs  10s  1.00 iters/s\n",
      "\n",
      "running (12.0s), 02/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 02/10 VUs  10s  1.00 iters/s\n",
      "\n",
      "running (13.0s), 02/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 02/10 VUs  10s  1.00 iters/s\n",
      "\n",
      "running (14.0s), 02/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 02/10 VUs  10s  1.00 iters/s\n",
      "\n",
      "running (15.0s), 02/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 02/10 VUs  10s  1.00 iters/s\n",
      "\n",
      "running (16.0s), 01/10 VUs, 9 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 02/10 VUs  10s  1.00 iters/s\n",
      "\n",
      "running (16.7s), 00/10 VUs, 10 complete and 0 interrupted iterations\n",
      "load_test ✓ [ 100% ] 00/10 VUs  10s  1.00 iters/s\n",
      "Saving scenario 79f58a9a-a2b8-47d7-a2ca-a4d60fc19eb4 results\n",
      "Scenario 79f58a9a-a2b8-47d7-a2ca-a4d60fc19eb4 complete\n"
     ]
    }
   ],
   "source": [
    "scenario.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'79f58a9a-a2b8-47d7-a2ca-a4d60fc19eb4'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario.scenario_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autobench.runner import BenchmarkRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_runner = BenchmarkRunner(\n",
    "    deployment=deployment, benchmark_dataset=benchmark_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ehdndzfif3v55m3w.us-east-1.aws.endpoints.huggingface.cloud'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment.endpoint.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark for deployment d30f3a82-95b9-44e8-b0ca-617d17cc\n",
      "Running benchmark for arrival rate 1\n",
      "Preparing scenario fa6befa6-cc3a-43eb-b298-b5f301ca9a6a\n",
      "Running scenario fa6befa6-cc3a-43eb-b298-b5f301ca9a6a\n",
      "\n",
      "          /\\      |‾‾| /‾‾/   /‾‾/   \n",
      "     /\\  /  \\     |  |/  /   /  /    \n",
      "    /  \\/    \\    |     (   /   ‾‾\\  \n",
      "   /          \\   |  |\\  \\ |  (‾)  | \n",
      "  / __________ \\  |__| \\__\\ \\_____/ .io\n",
      "\n",
      "     execution: local\n",
      "        script: /var/folders/w0/6t9rxkj97rv47l9sc0q22yth0000gn/T/autobench_uky___vy_k6_script.js\n",
      "        output: json (/Users/andrewreed/Documents/success_projects/auto-bench/autobench/benchmark_results/deployment_d30f3a82-95b9-44e8-b0ca-617d17cc/scenario_fa6befa6-cc3a-43eb-b298-b5f301ca9a6a/results.json)\n",
      "\n",
      "     scenarios: (100.00%) 1 scenario, 50 max VUs, 35s max duration (incl. graceful stop):\n",
      "              * load_test: 1.00 iterations/s for 5s (maxVUs: 50, gracefulStop: 30s)\n",
      "\n",
      "\n",
      "running (00.9s), 01/50 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  17% ] 01/50 VUs  0.9s/5s  1.00 iters/s\n",
      "\n",
      "running (01.9s), 02/50 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  37% ] 02/50 VUs  1.9s/5s  1.00 iters/s\n",
      "\n",
      "running (02.9s), 03/50 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  57% ] 03/50 VUs  2.9s/5s  1.00 iters/s\n",
      "\n",
      "running (03.9s), 04/50 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  77% ] 04/50 VUs  3.9s/5s  1.00 iters/s\n",
      "\n",
      "running (04.9s), 04/50 VUs, 1 complete and 0 interrupted iterations\n",
      "load_test   [  97% ] 04/50 VUs  4.9s/5s  1.00 iters/s\n",
      "\n",
      "running (05.9s), 04/50 VUs, 2 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 05/50 VUs  5s  1.00 iters/s\n",
      "\n",
      "running (06.9s), 02/50 VUs, 4 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 05/50 VUs  5s  1.00 iters/s\n",
      "\n",
      "running (07.9s), 02/50 VUs, 4 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 05/50 VUs  5s  1.00 iters/s\n",
      "\n",
      "running (07.9s), 00/50 VUs, 6 complete and 0 interrupted iterations\n",
      "load_test ✓ [ 100% ] 00/50 VUs  5s  1.00 iters/s\n",
      "Saving scenario fa6befa6-cc3a-43eb-b298-b5f301ca9a6a results\n",
      "Scenario fa6befa6-cc3a-43eb-b298-b5f301ca9a6a complete\n",
      "Benchmark for arrival rate 1 complete\n",
      "Running benchmark for arrival rate 10\n",
      "Preparing scenario 6cf80573-bdbf-4f73-b537-2ee20e32cd6d\n",
      "Running scenario 6cf80573-bdbf-4f73-b537-2ee20e32cd6d\n",
      "\n",
      "          /\\      |‾‾| /‾‾/   /‾‾/   \n",
      "     /\\  /  \\     |  |/  /   /  /    \n",
      "    /  \\/    \\    |     (   /   ‾‾\\  \n",
      "   /          \\   |  |\\  \\ |  (‾)  | \n",
      "  / __________ \\  |__| \\__\\ \\_____/ .io\n",
      "\n",
      "     execution: local\n",
      "        script: /var/folders/w0/6t9rxkj97rv47l9sc0q22yth0000gn/T/autobench_9d4x038e_k6_script.js\n",
      "        output: json (/Users/andrewreed/Documents/success_projects/auto-bench/autobench/benchmark_results/deployment_d30f3a82-95b9-44e8-b0ca-617d17cc/scenario_6cf80573-bdbf-4f73-b537-2ee20e32cd6d/results.json)\n",
      "\n",
      "     scenarios: (100.00%) 1 scenario, 50 max VUs, 35s max duration (incl. graceful stop):\n",
      "              * load_test: 10.00 iterations/s for 5s (maxVUs: 50, gracefulStop: 30s)\n",
      "\n",
      "\n",
      "running (00.9s), 09/50 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  17% ] 09/50 VUs  0.9s/5s  10.00 iters/s\n",
      "\n",
      "running (01.9s), 17/50 VUs, 2 complete and 0 interrupted iterations\n",
      "load_test   [  37% ] 17/50 VUs  1.9s/5s  10.00 iters/s\n",
      "\n",
      "running (02.9s), 23/50 VUs, 6 complete and 0 interrupted iterations\n",
      "load_test   [  57% ] 23/50 VUs  2.9s/5s  10.00 iters/s\n",
      "\n",
      "running (03.9s), 27/50 VUs, 12 complete and 0 interrupted iterations\n",
      "load_test   [  77% ] 27/50 VUs  3.9s/5s  10.00 iters/s\n",
      "\n",
      "running (04.9s), 32/50 VUs, 17 complete and 0 interrupted iterations\n",
      "load_test   [  97% ] 32/50 VUs  4.9s/5s  10.00 iters/s\n",
      "\n",
      "running (05.9s), 29/50 VUs, 21 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (06.9s), 24/50 VUs, 26 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (07.9s), 17/50 VUs, 33 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (08.9s), 13/50 VUs, 37 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (09.9s), 10/50 VUs, 40 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (10.9s), 06/50 VUs, 44 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (11.9s), 05/50 VUs, 45 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (12.9s), 01/50 VUs, 49 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 33/50 VUs  5s  10.00 iters/s\n",
      "\n",
      "running (13.1s), 00/50 VUs, 50 complete and 0 interrupted iterations\n",
      "load_test ✓ [ 100% ] 00/50 VUs  5s  10.00 iters/s\n",
      "Saving scenario 6cf80573-bdbf-4f73-b537-2ee20e32cd6d results\n",
      "Scenario 6cf80573-bdbf-4f73-b537-2ee20e32cd6d complete\n",
      "Benchmark for arrival rate 10 complete\n",
      "Deployment details saved to /Users/andrewreed/Documents/success_projects/auto-bench/autobench/benchmark_results/deployment_d30f3a82-95b9-44e8-b0ca-617d17cc/deployment_details.json\n",
      "Benchmark for deployment d30f3a82-95b9-44e8-b0ca-617d17cc complete\n"
     ]
    }
   ],
   "source": [
    "benchmark_runner.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrewrreed'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_api._get_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api.list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.constants import INFERENCE_ENDPOINTS_ENDPOINT\n",
    "from huggingface_hub.utils import get_session, hf_raise_for_status, build_hf_headers\n",
    "\n",
    "\n",
    "def fetch_quotas(namespace: str):\n",
    "    session = get_session()\n",
    "    response = session.get(\n",
    "        f\"{INFERENCE_ENDPOINTS_ENDPOINT}/provider/quotas/{namespace}\",\n",
    "        headers=build_hf_headers(),\n",
    "    )\n",
    "    hf_raise_for_status(response)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "quota = fetch_quotas(namespace=\"andrewrreed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vendors': [{'name': 'aws',\n",
       "   'quotas': [{'instanceType': 'nvidia-a10g',\n",
       "     'architecture': 'Nvidia A10G',\n",
       "     'maxAccelerators': 8,\n",
       "     'usedAccelerators': 3},\n",
       "    {'instanceType': 'nvidia-l4',\n",
       "     'architecture': 'Nvidia L4',\n",
       "     'maxAccelerators': 8,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'nvidia-a100',\n",
       "     'architecture': 'Nvidia A100',\n",
       "     'maxAccelerators': 2,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'intel-icl',\n",
       "     'architecture': 'Intel Ice Lake',\n",
       "     'maxAccelerators': 40,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'nvidia-t4',\n",
       "     'architecture': 'Nvidia T4',\n",
       "     'maxAccelerators': 15,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'intel-spr',\n",
       "     'architecture': 'Intel Sapphire Rapids',\n",
       "     'maxAccelerators': 40,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'inf2',\n",
       "     'architecture': 'AWS Inferentia 2',\n",
       "     'maxAccelerators': 24,\n",
       "     'usedAccelerators': 0}]},\n",
       "  {'name': 'azure',\n",
       "   'quotas': [{'instanceType': 'intel-xeon',\n",
       "     'architecture': 'Intel Xeon',\n",
       "     'maxAccelerators': 40,\n",
       "     'usedAccelerators': 0}]},\n",
       "  {'name': 'gcp',\n",
       "   'quotas': [{'instanceType': 'v5e',\n",
       "     'architecture': 'Google Cloud TPU',\n",
       "     'maxAccelerators': 8,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'nvidia-l4',\n",
       "     'architecture': 'Nvidia L4',\n",
       "     'maxAccelerators': 8,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'intel-spr',\n",
       "     'architecture': 'Intel Sapphire Rapids',\n",
       "     'maxAccelerators': 40,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'nvidia-h100',\n",
       "     'architecture': 'Nvidia H100',\n",
       "     'maxAccelerators': 2,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'nvidia-a100',\n",
       "     'architecture': 'Nvidia A100',\n",
       "     'maxAccelerators': 2,\n",
       "     'usedAccelerators': 0},\n",
       "    {'instanceType': 'nvidia-t4',\n",
       "     'architecture': 'Nvidia T4',\n",
       "     'maxAccelerators': 5,\n",
       "     'usedAccelerators': 0}]}]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aScheduler design:\n",
    "\n",
    "Needs to:\n",
    "\n",
    "1. Take in:\n",
    "   - model_id\n",
    "   - GPU types\n",
    "   - Optionally, provider (AWS/GCP)\n",
    "   - Optionally, specify (MAX_INPUT_TOKENS, MAX_TOTAL_TOKENS, MAX_BATCH_PREFILL_TOKENS, MAX_BATCH_TOTAL_TOKENS)\n",
    "2. Determine which instances can fit the desired model\n",
    "3. Iterate through instances to:\n",
    "   a. deploy\n",
    "   b. benchmark\n",
    "   c. report results\n",
    "\n",
    "Runner design:\n",
    "\n",
    "1. Take in model_id, instance, TGI config\n",
    "2. Deploy this\n",
    "3. Shut down / pause instance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
