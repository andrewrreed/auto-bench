{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tests with K6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Lookup compatible configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get IE compute instance options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autobench.utils import ComputeOptionUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor</th>\n",
       "      <th>vendor_status</th>\n",
       "      <th>region</th>\n",
       "      <th>region_label</th>\n",
       "      <th>region_status</th>\n",
       "      <th>id</th>\n",
       "      <th>accelerator</th>\n",
       "      <th>num_gpus</th>\n",
       "      <th>memory_in_gb</th>\n",
       "      <th>gpu_memory_in_gb</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_size</th>\n",
       "      <th>architecture</th>\n",
       "      <th>status</th>\n",
       "      <th>price_per_hour</th>\n",
       "      <th>num_cpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-t4-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>nvidia-t4</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia T4</td>\n",
       "      <td>available</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-t4-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>192</td>\n",
       "      <td>64</td>\n",
       "      <td>nvidia-t4</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia T4</td>\n",
       "      <td>available</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a10g-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>nvidia-a10g</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia A10G</td>\n",
       "      <td>available</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a10g-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>186</td>\n",
       "      <td>96</td>\n",
       "      <td>nvidia-a10g</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia A10G</td>\n",
       "      <td>available</td>\n",
       "      <td>5.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a100-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>80</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a100-x2</td>\n",
       "      <td>gpu</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>160</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x2</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a100-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>580</td>\n",
       "      <td>320</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>16.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-a100-x8</td>\n",
       "      <td>gpu</td>\n",
       "      <td>8</td>\n",
       "      <td>1160</td>\n",
       "      <td>640</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x8</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-l4-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>185</td>\n",
       "      <td>96</td>\n",
       "      <td>nvidia-l4</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia L4</td>\n",
       "      <td>available</td>\n",
       "      <td>3.8</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>N. Virginia</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-us-east-1-nvidia-l4-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>nvidia-l4</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia L4</td>\n",
       "      <td>available</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>eu-west-1</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-eu-west-1-nvidia-t4-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>nvidia-t4</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia T4</td>\n",
       "      <td>available</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>aws</td>\n",
       "      <td>available</td>\n",
       "      <td>eu-west-1</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>available</td>\n",
       "      <td>aws-eu-west-1-nvidia-a10g-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>nvidia-a10g</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia A10G</td>\n",
       "      <td>available</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-t4-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>nvidia-t4</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia T4</td>\n",
       "      <td>available</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-l4-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>nvidia-l4</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia L4</td>\n",
       "      <td>available</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-l4-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>186</td>\n",
       "      <td>96</td>\n",
       "      <td>nvidia-l4</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia L4</td>\n",
       "      <td>available</td>\n",
       "      <td>5.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-a100-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>80</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-a100-x2</td>\n",
       "      <td>gpu</td>\n",
       "      <td>2</td>\n",
       "      <td>348</td>\n",
       "      <td>160</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x2</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-a100-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>696</td>\n",
       "      <td>320</td>\n",
       "      <td>nvidia-a100</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia A100</td>\n",
       "      <td>available</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-h100-x1</td>\n",
       "      <td>gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>80</td>\n",
       "      <td>nvidia-h100</td>\n",
       "      <td>x1</td>\n",
       "      <td>Nvidia H100</td>\n",
       "      <td>available</td>\n",
       "      <td>12.5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-h100-x2</td>\n",
       "      <td>gpu</td>\n",
       "      <td>2</td>\n",
       "      <td>480</td>\n",
       "      <td>160</td>\n",
       "      <td>nvidia-h100</td>\n",
       "      <td>x2</td>\n",
       "      <td>Nvidia H100</td>\n",
       "      <td>available</td>\n",
       "      <td>25.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gcp</td>\n",
       "      <td>available</td>\n",
       "      <td>us-east4</td>\n",
       "      <td>US East 4</td>\n",
       "      <td>available</td>\n",
       "      <td>gcp-us-east4-nvidia-h100-x4</td>\n",
       "      <td>gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>960</td>\n",
       "      <td>320</td>\n",
       "      <td>nvidia-h100</td>\n",
       "      <td>x4</td>\n",
       "      <td>Nvidia H100</td>\n",
       "      <td>available</td>\n",
       "      <td>50.0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor vendor_status     region region_label region_status  \\\n",
       "0     aws     available  us-east-1  N. Virginia     available   \n",
       "1     aws     available  us-east-1  N. Virginia     available   \n",
       "2     aws     available  us-east-1  N. Virginia     available   \n",
       "3     aws     available  us-east-1  N. Virginia     available   \n",
       "4     aws     available  us-east-1  N. Virginia     available   \n",
       "5     aws     available  us-east-1  N. Virginia     available   \n",
       "6     aws     available  us-east-1  N. Virginia     available   \n",
       "7     aws     available  us-east-1  N. Virginia     available   \n",
       "8     aws     available  us-east-1  N. Virginia     available   \n",
       "9     aws     available  us-east-1  N. Virginia     available   \n",
       "10    aws     available  eu-west-1      Ireland     available   \n",
       "11    aws     available  eu-west-1      Ireland     available   \n",
       "12    gcp     available   us-east4    US East 4     available   \n",
       "13    gcp     available   us-east4    US East 4     available   \n",
       "14    gcp     available   us-east4    US East 4     available   \n",
       "15    gcp     available   us-east4    US East 4     available   \n",
       "16    gcp     available   us-east4    US East 4     available   \n",
       "17    gcp     available   us-east4    US East 4     available   \n",
       "18    gcp     available   us-east4    US East 4     available   \n",
       "19    gcp     available   us-east4    US East 4     available   \n",
       "20    gcp     available   us-east4    US East 4     available   \n",
       "\n",
       "                              id accelerator  num_gpus  memory_in_gb  \\\n",
       "0     aws-us-east-1-nvidia-t4-x1         gpu         1            15   \n",
       "1     aws-us-east-1-nvidia-t4-x4         gpu         4           192   \n",
       "2   aws-us-east-1-nvidia-a10g-x1         gpu         1            30   \n",
       "3   aws-us-east-1-nvidia-a10g-x4         gpu         4           186   \n",
       "4   aws-us-east-1-nvidia-a100-x1         gpu         1           145   \n",
       "5   aws-us-east-1-nvidia-a100-x2         gpu         2           290   \n",
       "6   aws-us-east-1-nvidia-a100-x4         gpu         4           580   \n",
       "7   aws-us-east-1-nvidia-a100-x8         gpu         8          1160   \n",
       "8     aws-us-east-1-nvidia-l4-x4         gpu         4           185   \n",
       "9     aws-us-east-1-nvidia-l4-x1         gpu         1            30   \n",
       "10    aws-eu-west-1-nvidia-t4-x1         gpu         1            15   \n",
       "11  aws-eu-west-1-nvidia-a10g-x1         gpu         1            30   \n",
       "12     gcp-us-east4-nvidia-t4-x1         gpu         1            11   \n",
       "13     gcp-us-east4-nvidia-l4-x1         gpu         1            12   \n",
       "14     gcp-us-east4-nvidia-l4-x4         gpu         4           186   \n",
       "15   gcp-us-east4-nvidia-a100-x1         gpu         1           174   \n",
       "16   gcp-us-east4-nvidia-a100-x2         gpu         2           348   \n",
       "17   gcp-us-east4-nvidia-a100-x4         gpu         4           696   \n",
       "18   gcp-us-east4-nvidia-h100-x1         gpu         1           240   \n",
       "19   gcp-us-east4-nvidia-h100-x2         gpu         2           480   \n",
       "20   gcp-us-east4-nvidia-h100-x4         gpu         4           960   \n",
       "\n",
       "    gpu_memory_in_gb instance_type instance_size architecture     status  \\\n",
       "0                 16     nvidia-t4            x1    Nvidia T4  available   \n",
       "1                 64     nvidia-t4            x4    Nvidia T4  available   \n",
       "2                 24   nvidia-a10g            x1  Nvidia A10G  available   \n",
       "3                 96   nvidia-a10g            x4  Nvidia A10G  available   \n",
       "4                 80   nvidia-a100            x1  Nvidia A100  available   \n",
       "5                160   nvidia-a100            x2  Nvidia A100  available   \n",
       "6                320   nvidia-a100            x4  Nvidia A100  available   \n",
       "7                640   nvidia-a100            x8  Nvidia A100  available   \n",
       "8                 96     nvidia-l4            x4    Nvidia L4  available   \n",
       "9                 24     nvidia-l4            x1    Nvidia L4  available   \n",
       "10                16     nvidia-t4            x1    Nvidia T4  available   \n",
       "11                24   nvidia-a10g            x1  Nvidia A10G  available   \n",
       "12                16     nvidia-t4            x1    Nvidia T4  available   \n",
       "13                24     nvidia-l4            x1    Nvidia L4  available   \n",
       "14                96     nvidia-l4            x4    Nvidia L4  available   \n",
       "15                80   nvidia-a100            x1  Nvidia A100  available   \n",
       "16               160   nvidia-a100            x2  Nvidia A100  available   \n",
       "17               320   nvidia-a100            x4  Nvidia A100  available   \n",
       "18                80   nvidia-h100            x1  Nvidia H100  available   \n",
       "19               160   nvidia-h100            x2  Nvidia H100  available   \n",
       "20               320   nvidia-h100            x4  Nvidia H100  available   \n",
       "\n",
       "    price_per_hour  num_cpus  \n",
       "0              0.5         3  \n",
       "1              3.0        46  \n",
       "2              1.0         6  \n",
       "3              5.0        46  \n",
       "4              4.0        11  \n",
       "5              8.0        22  \n",
       "6             16.0        44  \n",
       "7             32.0        88  \n",
       "8              3.8        47  \n",
       "9              0.8         7  \n",
       "10             0.5         3  \n",
       "11             1.0         6  \n",
       "12             0.5         3  \n",
       "13             1.0         3  \n",
       "14             5.0        46  \n",
       "15             6.0        11  \n",
       "16            12.0        23  \n",
       "17            24.0        47  \n",
       "18            12.5        25  \n",
       "19            25.0        51  \n",
       "20            50.0       102  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_option_util = ComputeOptionUtil()\n",
    "compute_option_util.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   vendor            21 non-null     object \n",
      " 1   vendor_status     21 non-null     object \n",
      " 2   region            21 non-null     object \n",
      " 3   region_label      21 non-null     object \n",
      " 4   region_status     21 non-null     object \n",
      " 5   id                21 non-null     object \n",
      " 6   accelerator       21 non-null     object \n",
      " 7   num_gpus          21 non-null     int64  \n",
      " 8   memory_in_gb      21 non-null     int64  \n",
      " 9   gpu_memory_in_gb  21 non-null     int64  \n",
      " 10  instance_type     21 non-null     object \n",
      " 11  instance_size     21 non-null     object \n",
      " 12  architecture      21 non-null     object \n",
      " 13  status            21 non-null     object \n",
      " 14  price_per_hour    21 non-null     float64\n",
      " 15  num_cpus          21 non-null     int64  \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "compute_option_util.options.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User specifies their desired inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR = \"aws\"\n",
    "REGION = \"us-east-1\"\n",
    "GPU_TYPES = [\"nvidia-a10g\", \"nvidia-l4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_instances = compute_option_util.get_instance_details(\n",
    "    vendor=VENDOR, region=REGION, gpu_types=GPU_TYPES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vendor': 'aws',\n",
       "  'vendor_status': 'available',\n",
       "  'region': 'us-east-1',\n",
       "  'region_label': 'N. Virginia',\n",
       "  'region_status': 'available',\n",
       "  'id': 'aws-us-east-1-nvidia-a10g-x1',\n",
       "  'accelerator': 'gpu',\n",
       "  'num_gpus': 1,\n",
       "  'memory_in_gb': 30,\n",
       "  'gpu_memory_in_gb': 24,\n",
       "  'instance_type': 'nvidia-a10g',\n",
       "  'instance_size': 'x1',\n",
       "  'architecture': 'Nvidia A10G',\n",
       "  'status': 'available',\n",
       "  'price_per_hour': 1.0,\n",
       "  'num_cpus': 6},\n",
       " {'vendor': 'aws',\n",
       "  'vendor_status': 'available',\n",
       "  'region': 'us-east-1',\n",
       "  'region_label': 'N. Virginia',\n",
       "  'region_status': 'available',\n",
       "  'id': 'aws-us-east-1-nvidia-a10g-x4',\n",
       "  'accelerator': 'gpu',\n",
       "  'num_gpus': 4,\n",
       "  'memory_in_gb': 186,\n",
       "  'gpu_memory_in_gb': 96,\n",
       "  'instance_type': 'nvidia-a10g',\n",
       "  'instance_size': 'x4',\n",
       "  'architecture': 'Nvidia A10G',\n",
       "  'status': 'available',\n",
       "  'price_per_hour': 5.0,\n",
       "  'num_cpus': 46},\n",
       " {'vendor': 'aws',\n",
       "  'vendor_status': 'available',\n",
       "  'region': 'us-east-1',\n",
       "  'region_label': 'N. Virginia',\n",
       "  'region_status': 'available',\n",
       "  'id': 'aws-us-east-1-nvidia-l4-x4',\n",
       "  'accelerator': 'gpu',\n",
       "  'num_gpus': 4,\n",
       "  'memory_in_gb': 185,\n",
       "  'gpu_memory_in_gb': 96,\n",
       "  'instance_type': 'nvidia-l4',\n",
       "  'instance_size': 'x4',\n",
       "  'architecture': 'Nvidia L4',\n",
       "  'status': 'available',\n",
       "  'price_per_hour': 3.8,\n",
       "  'num_cpus': 47},\n",
       " {'vendor': 'aws',\n",
       "  'vendor_status': 'available',\n",
       "  'region': 'us-east-1',\n",
       "  'region_label': 'N. Virginia',\n",
       "  'region_status': 'available',\n",
       "  'id': 'aws-us-east-1-nvidia-l4-x1',\n",
       "  'accelerator': 'gpu',\n",
       "  'num_gpus': 1,\n",
       "  'memory_in_gb': 30,\n",
       "  'gpu_memory_in_gb': 24,\n",
       "  'instance_type': 'nvidia-l4',\n",
       "  'instance_size': 'x1',\n",
       "  'architecture': 'Nvidia L4',\n",
       "  'status': 'available',\n",
       "  'price_per_hour': 0.8,\n",
       "  'num_cpus': 7}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, check if model will work on any of the desired instances, and if so, get each TGI config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "viable_instances = compute_option_util.get_viable_instance_configs(\n",
    "    model_id=model_id, instances=possible_instances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tgi_config': {'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "   'max_batch_prefill_tokens': 8192,\n",
       "   'max_input_length': 6000,\n",
       "   'max_total_tokens': 6144,\n",
       "   'num_shard': 1,\n",
       "   'quantize': None,\n",
       "   'estimated_memory_in_gigabytes': 23.01},\n",
       "  'instance_config': {'vendor': 'aws',\n",
       "   'vendor_status': 'available',\n",
       "   'region': 'us-east-1',\n",
       "   'region_label': 'N. Virginia',\n",
       "   'region_status': 'available',\n",
       "   'id': 'aws-us-east-1-nvidia-a10g-x1',\n",
       "   'accelerator': 'gpu',\n",
       "   'num_gpus': 1,\n",
       "   'memory_in_gb': 30,\n",
       "   'gpu_memory_in_gb': 24,\n",
       "   'instance_type': 'nvidia-a10g',\n",
       "   'instance_size': 'x1',\n",
       "   'architecture': 'Nvidia A10G',\n",
       "   'status': 'available',\n",
       "   'price_per_hour': 1.0,\n",
       "   'num_cpus': 6}},\n",
       " {'tgi_config': {'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "   'max_batch_prefill_tokens': 32768,\n",
       "   'max_input_length': 6000,\n",
       "   'max_total_tokens': 6144,\n",
       "   'num_shard': 4,\n",
       "   'quantize': None,\n",
       "   'estimated_memory_in_gigabytes': 98.36},\n",
       "  'instance_config': {'vendor': 'aws',\n",
       "   'vendor_status': 'available',\n",
       "   'region': 'us-east-1',\n",
       "   'region_label': 'N. Virginia',\n",
       "   'region_status': 'available',\n",
       "   'id': 'aws-us-east-1-nvidia-a10g-x4',\n",
       "   'accelerator': 'gpu',\n",
       "   'num_gpus': 4,\n",
       "   'memory_in_gb': 186,\n",
       "   'gpu_memory_in_gb': 96,\n",
       "   'instance_type': 'nvidia-a10g',\n",
       "   'instance_size': 'x4',\n",
       "   'architecture': 'Nvidia A10G',\n",
       "   'status': 'available',\n",
       "   'price_per_hour': 5.0,\n",
       "   'num_cpus': 46}},\n",
       " {'tgi_config': {'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "   'max_batch_prefill_tokens': 32768,\n",
       "   'max_input_length': 6000,\n",
       "   'max_total_tokens': 6144,\n",
       "   'num_shard': 4,\n",
       "   'quantize': None,\n",
       "   'estimated_memory_in_gigabytes': 98.36},\n",
       "  'instance_config': {'vendor': 'aws',\n",
       "   'vendor_status': 'available',\n",
       "   'region': 'us-east-1',\n",
       "   'region_label': 'N. Virginia',\n",
       "   'region_status': 'available',\n",
       "   'id': 'aws-us-east-1-nvidia-l4-x4',\n",
       "   'accelerator': 'gpu',\n",
       "   'num_gpus': 4,\n",
       "   'memory_in_gb': 185,\n",
       "   'gpu_memory_in_gb': 96,\n",
       "   'instance_type': 'nvidia-l4',\n",
       "   'instance_size': 'x4',\n",
       "   'architecture': 'Nvidia L4',\n",
       "   'status': 'available',\n",
       "   'price_per_hour': 3.8,\n",
       "   'num_cpus': 47}},\n",
       " {'tgi_config': {'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "   'max_batch_prefill_tokens': 8192,\n",
       "   'max_input_length': 6000,\n",
       "   'max_total_tokens': 6144,\n",
       "   'num_shard': 1,\n",
       "   'quantize': None,\n",
       "   'estimated_memory_in_gigabytes': 23.01},\n",
       "  'instance_config': {'vendor': 'aws',\n",
       "   'vendor_status': 'available',\n",
       "   'region': 'us-east-1',\n",
       "   'region_label': 'N. Virginia',\n",
       "   'region_status': 'available',\n",
       "   'id': 'aws-us-east-1-nvidia-l4-x1',\n",
       "   'accelerator': 'gpu',\n",
       "   'num_gpus': 1,\n",
       "   'memory_in_gb': 30,\n",
       "   'gpu_memory_in_gb': 24,\n",
       "   'instance_type': 'nvidia-l4',\n",
       "   'instance_size': 'x1',\n",
       "   'architecture': 'Nvidia L4',\n",
       "   'status': 'available',\n",
       "   'price_per_hour': 0.8,\n",
       "   'num_cpus': 7}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viable_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tgi_config': {'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "  'max_batch_prefill_tokens': 8192,\n",
       "  'max_input_length': 6000,\n",
       "  'max_total_tokens': 6144,\n",
       "  'num_shard': 1,\n",
       "  'quantize': None,\n",
       "  'estimated_memory_in_gigabytes': 23.01},\n",
       " 'instance_config': {'vendor': 'aws',\n",
       "  'vendor_status': 'available',\n",
       "  'region': 'us-east-1',\n",
       "  'region_label': 'N. Virginia',\n",
       "  'region_status': 'available',\n",
       "  'id': 'aws-us-east-1-nvidia-a10g-x1',\n",
       "  'accelerator': 'gpu',\n",
       "  'num_gpus': 1,\n",
       "  'memory_in_gb': 30,\n",
       "  'gpu_memory_in_gb': 24,\n",
       "  'instance_type': 'nvidia-a10g',\n",
       "  'instance_size': 'x1',\n",
       "  'architecture': 'Nvidia A10G',\n",
       "  'status': 'available',\n",
       "  'price_per_hour': 1.0,\n",
       "  'num_cpus': 6}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viable_instances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deploy LLM with TGI on Inference Endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewreed/Documents/success_projects/auto-bench/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autobench.deploy import IEDeployment\n",
    "from autobench.utils import TGIConfig, ComputeInstanceConfig, ComputeOptionUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "viable_instance = viable_instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tgi_config': {'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "  'max_batch_prefill_tokens': 8192,\n",
       "  'max_input_length': 6000,\n",
       "  'max_total_tokens': 6144,\n",
       "  'num_shard': 1,\n",
       "  'quantize': None,\n",
       "  'estimated_memory_in_gigabytes': 23.01},\n",
       " 'instance_config': {'vendor': 'aws',\n",
       "  'vendor_status': 'available',\n",
       "  'region': 'us-east-1',\n",
       "  'region_label': 'N. Virginia',\n",
       "  'region_status': 'available',\n",
       "  'id': 'aws-us-east-1-nvidia-a10g-x1',\n",
       "  'accelerator': 'gpu',\n",
       "  'num_gpus': 1,\n",
       "  'memory_in_gb': 30,\n",
       "  'gpu_memory_in_gb': 24,\n",
       "  'instance_type': 'nvidia-a10g',\n",
       "  'instance_size': 'x1',\n",
       "  'architecture': 'Nvidia A10G',\n",
       "  'status': 'available',\n",
       "  'price_per_hour': 1.0,\n",
       "  'num_cpus': 6}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viable_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgi_config = TGIConfig(**viable_instance[\"tgi_config\"])\n",
    "instance_config = ComputeInstanceConfig(**viable_instance[\"instance_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAX_BATCH_PREFILL_TOKENS': '8192',\n",
       " 'MAX_INPUT_LENGTH': '6000',\n",
       " 'MAX_TOTAL_TOKENS': '6144',\n",
       " 'NUM_SHARD': '1',\n",
       " 'MODEL_ID': '/repository'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgi_config.env_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = IEDeployment(tgi_config=tgi_config, instance_config=instance_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGIConfig(model_id='meta-llama/Meta-Llama-3-8B-Instruct', max_batch_prefill_tokens=8192, max_input_length=6000, max_total_tokens=6144, num_shard=1, quantize=None, estimated_memory_in_gigabytes=23.01)\n"
     ]
    }
   ],
   "source": [
    "print(deployment.tgi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobench'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating inference endpoint...\n",
      "Endpoint created successfully: https://v1wfvykisrysn6e0.us-east-1.aws.endpoints.huggingface.cloud\n"
     ]
    }
   ],
   "source": [
    "deployment.deploy_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://v1wfvykisrysn6e0.us-east-1.aws.endpoints.huggingface.cloud'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment.endpoint.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare ShareGPT data for realisitic inference workload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1999/2000 [00:00<00:00, 47939.97it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"Open-Orca/slimorca-deduped-cleaned-corrected\", split=\"train\"\n",
    ")\n",
    "\n",
    "# Select only the first 2k conversations\n",
    "max = min(2000, len(dataset))\n",
    "conversations = []\n",
    "\n",
    "for item in tqdm.tqdm(dataset, total=max):\n",
    "    conv = item.get(\"conversations\")\n",
    "    if conv and conv[0][\"from\"] == \"system\":\n",
    "        # Get only the initial user message\n",
    "        conv = conv[1:2]\n",
    "        conversations.append(conv)\n",
    "\n",
    "        if len(conversations) >= max:\n",
    "            break\n",
    "\n",
    "\n",
    "with open(\"..benchmark_data/small.json\", \"w\") as f:\n",
    "    data = json.dump(conversations, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run K6 Load Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, install the openai Python library by running\n",
    "# pip install openai\n",
    "\n",
    "import requests\n",
    "from huggingface_hub import get_token\n",
    "\n",
    "\n",
    "API_URL = f\"{deployment.endpoint.url}\" + \"/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {get_token()}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response\n",
    "\n",
    "\n",
    "output = query(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"How old is Statute of Liberty?\"}],\n",
    "        \"max_tokens\": 150,\n",
    "        \"model\": \"tgi\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1724350025,\"model\":\"/repository\",\"system_fingerprint\":\"2.2.1-dev0-sha-358ceb6\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The Statue of Liberty, also known as Lady Liberty, was dedicated on October 28, 1886. It was designed by French sculptor Fr\\xc3\\xa9d\\xc3\\xa9ric Auguste Bartholdi and built by Gustave Eiffel. The statue was a gift from the people of France to the people of the United States to commemorate the 100th anniversary of American independence.\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":18,\"completion_tokens\":75,\"total_tokens\":93}}'\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Thu, 22 Aug 2024 18:07:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '626', 'Connection': 'keep-alive', 'x-compute-type': '1-nvidia-a10g', 'x-compute-time': '2.556334947', 'x-compute-characters': '146', 'x-total-time': '2556', 'x-validation-time': '0', 'x-queue-time': '0', 'x-inference-time': '2555', 'x-time-per-token': '34', 'x-prompt-tokens': '19', 'x-generated-tokens': '75', 'vary': 'origin, access-control-request-method, access-control-request-headers, origin, access-control-request-method, access-control-request-headers', 'access-control-allow-origin': '*', 'x-proxied-host': 'http://10.101.113.22', 'x-proxied-path': '/v1/chat/completions', 'x-request-id': 'TXh2Sy', 'access-control-allow-credentials': 'true'}\n"
     ]
    }
   ],
   "source": [
    "print(output.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autobench.k6 import K6Config, K6ConstantArrivalRateExecutor, K6Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = K6ConstantArrivalRateExecutor(\n",
    "    pre_allocated_vus=10, rate_per_second=1, duration=\"20s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = K6Config(\n",
    "    host=deployment.endpoint.url,\n",
    "    executor=executor,\n",
    "    data_file=\"small.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K6Config(url=https://v1wfvykisrysn6e0.us-east-1.aws.endpoints.huggingface.cloud executor=<autobench.k6.K6ConstantArrivalRateExecutor object at 0x168febd60> data_file=small.json)\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre_allocated_vus': 10,\n",
       " 'rate': 1,\n",
       " 'duration': '20s',\n",
       " 'host': 'https://v1wfvykisrysn6e0.us-east-1.aws.endpoints.huggingface.cloud',\n",
       " 'data_file': 'small.json',\n",
       " 'data_path': '/Users/andrewreed/Documents/success_projects/auto-bench/autobench/benchmark_data',\n",
       " 'temp_dir': '/var/folders/w0/6t9rxkj97rv47l9sc0q22yth0000gn/T/autobench_d_z_xlud_k6_results'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.executor.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = K6Benchmark(config=config, output_dir=\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://v1wfvykisrysn6e0.us-east-1.aws.endpoints.huggingface.cloud'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          /\\      |‾‾| /‾‾/   /‾‾/   \n",
      "     /\\  /  \\     |  |/  /   /  /    \n",
      "    /  \\/    \\    |     (   /   ‾‾\\  \n",
      "   /          \\   |  |\\  \\ |  (‾)  | \n",
      "  / __________ \\  |__| \\__\\ \\_____/ .io\n",
      "\n",
      "     execution: local\n",
      "        script: /var/folders/w0/6t9rxkj97rv47l9sc0q22yth0000gn/T/autobench_2cccx0phk6_script\n",
      "        output: json (/var/folders/w0/6t9rxkj97rv47l9sc0q22yth0000gn/T/autobench_d_z_xlud_k6_results/results.json)\n",
      "\n",
      "     scenarios: (100.00%) 1 scenario, 10 max VUs, 50s max duration (incl. graceful stop):\n",
      "              * load_test: 1.00 iterations/s for 20s (maxVUs: 10, gracefulStop: 30s)\n",
      "\n",
      "\n",
      "running (01.0s), 01/10 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [   5% ] 01/10 VUs  01.0s/20s  1.00 iters/s\n",
      "\n",
      "running (02.0s), 02/10 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  10% ] 02/10 VUs  02.0s/20s  1.00 iters/s\n",
      "\n",
      "running (03.0s), 03/10 VUs, 0 complete and 0 interrupted iterations\n",
      "load_test   [  15% ] 03/10 VUs  03.0s/20s  1.00 iters/s\n",
      "\n",
      "running (04.0s), 03/10 VUs, 1 complete and 0 interrupted iterations\n",
      "load_test   [  20% ] 03/10 VUs  04.0s/20s  1.00 iters/s\n",
      "\n",
      "running (05.0s), 04/10 VUs, 1 complete and 0 interrupted iterations\n",
      "load_test   [  25% ] 04/10 VUs  05.0s/20s  1.00 iters/s\n",
      "\n",
      "running (06.0s), 04/10 VUs, 2 complete and 0 interrupted iterations\n",
      "load_test   [  30% ] 04/10 VUs  06.0s/20s  1.00 iters/s\n",
      "\n",
      "running (07.0s), 03/10 VUs, 4 complete and 0 interrupted iterations\n",
      "load_test   [  35% ] 03/10 VUs  07.0s/20s  1.00 iters/s\n",
      "\n",
      "running (08.0s), 04/10 VUs, 4 complete and 0 interrupted iterations\n",
      "load_test   [  40% ] 04/10 VUs  08.0s/20s  1.00 iters/s\n",
      "\n",
      "running (09.0s), 01/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test   [  45% ] 01/10 VUs  09.0s/20s  1.00 iters/s\n",
      "\n",
      "running (10.0s), 02/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test   [  50% ] 02/10 VUs  10.0s/20s  1.00 iters/s\n",
      "\n",
      "running (11.0s), 03/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test   [  55% ] 03/10 VUs  11.0s/20s  1.00 iters/s\n",
      "\n",
      "running (12.0s), 04/10 VUs, 8 complete and 0 interrupted iterations\n",
      "load_test   [  60% ] 04/10 VUs  12.0s/20s  1.00 iters/s\n",
      "\n",
      "running (13.0s), 04/10 VUs, 9 complete and 0 interrupted iterations\n",
      "load_test   [  65% ] 04/10 VUs  13.0s/20s  1.00 iters/s\n",
      "\n",
      "running (14.0s), 05/10 VUs, 9 complete and 0 interrupted iterations\n",
      "load_test   [  70% ] 05/10 VUs  14.0s/20s  1.00 iters/s\n",
      "\n",
      "running (15.0s), 06/10 VUs, 9 complete and 0 interrupted iterations\n",
      "load_test   [  75% ] 06/10 VUs  15.0s/20s  1.00 iters/s\n",
      "\n",
      "running (16.0s), 05/10 VUs, 11 complete and 0 interrupted iterations\n",
      "load_test   [  80% ] 05/10 VUs  16.0s/20s  1.00 iters/s\n",
      "\n",
      "running (17.0s), 04/10 VUs, 13 complete and 0 interrupted iterations\n",
      "load_test   [  85% ] 04/10 VUs  17.0s/20s  1.00 iters/s\n",
      "\n",
      "running (18.0s), 04/10 VUs, 14 complete and 0 interrupted iterations\n",
      "load_test   [  90% ] 04/10 VUs  18.0s/20s  1.00 iters/s\n",
      "\n",
      "running (19.0s), 03/10 VUs, 16 complete and 0 interrupted iterations\n",
      "load_test   [  95% ] 03/10 VUs  19.0s/20s  1.00 iters/s\n",
      "\n",
      "running (20.0s), 04/10 VUs, 16 complete and 0 interrupted iterations\n",
      "load_test   [ 100% ] 04/10 VUs  20.0s/20s  1.00 iters/s\n",
      "\n",
      "running (21.0s), 02/10 VUs, 18 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 03/10 VUs  20s  1.00 iters/s\n",
      "\n",
      "running (22.0s), 01/10 VUs, 19 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 03/10 VUs  20s  1.00 iters/s\n",
      "\n",
      "running (23.0s), 01/10 VUs, 19 complete and 0 interrupted iterations\n",
      "load_test ↓ [ 100% ] 03/10 VUs  20s  1.00 iters/s\n",
      "\n",
      "running (23.8s), 00/10 VUs, 20 complete and 0 interrupted iterations\n",
      "load_test ✓ [ 100% ] 00/10 VUs  20s  1.00 iters/s\n"
     ]
    }
   ],
   "source": [
    "benchmark.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results.summary.json'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.get_summary_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aScheduler design:\n",
    "\n",
    "Needs to:\n",
    "\n",
    "1. Take in:\n",
    "   - model_id\n",
    "   - GPU types\n",
    "   - Optionally, provider (AWS/GCP)\n",
    "   - Optionally, specify (MAX_INPUT_TOKENS, MAX_TOTAL_TOKENS, MAX_BATCH_PREFILL_TOKENS, MAX_BATCH_TOTAL_TOKENS)\n",
    "2. Determine which instances can fit the desired model\n",
    "3. Iterate through instances to:\n",
    "   a. deploy\n",
    "   b. benchmark\n",
    "   c. report results\n",
    "\n",
    "Runner design:\n",
    "\n",
    "1. Take in model_id, instance, TGI config\n",
    "2. Deploy this\n",
    "3. Shut down / pause instance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
